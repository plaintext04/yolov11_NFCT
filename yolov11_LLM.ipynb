{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SlcEyOqCG04","outputId":"9baffab7-93ef-4a61-aeef-abdafdea9e13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.13\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7tVArzYCfdS","outputId":"7acb6868-49f4-44fa-a531-4486797eca97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My Drive"]},{"cell_type":"code","source":["!pip install transformers huggingface_hub datasets evaluate unsloth"],"metadata":{"id":"1DcpIP8kr-BI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install bitsandbytes ultralytics"],"metadata":{"id":"TPALmcfO2Bnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9F3ciOceDMz1"},"outputs":[],"source":["%cd /content/drive/My Drive/yolov11"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8gb3ib8hjVk"},"outputs":[],"source":["from unsloth import FastVisionModel\n","from ultralytics import YOLO\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","import gc\n","from IPython.display import display"]},{"cell_type":"code","source":["det_model = YOLO(\"runs/detect/train2/weights/best.pt\")"],"metadata":{"id":"ijZ5zDOo_KSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihN0AerY2pxy"},"outputs":[],"source":["model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\"\n","model, tokenizer = FastVisionModel.from_pretrained(\n","    model_name,\n","    load_in_4bit=True,\n","    use_gradient_checkpointing=\"unsloth\"\n",")\n","model.eval()\n"]},{"cell_type":"code","source":["def get_detections_with_pos(img_path: str):\n","    img = Image.open(img_path)\n","    w, h = img.size\n","    res = det_model(img_path)[0]\n","    labels = [det_model.names[int(c)] for c in res.boxes.cls]\n","    boxes  = [xyxy.tolist() for xyxy in res.boxes.xyxy]\n","\n","    result = []\n","    for lab, box in zip(labels, boxes):\n","        x1,y1,x2,y2 = box\n","        cx, cy = (x1+x2)/2, (y1+y2)/2\n","\n","        hor = \"left\" if cx < w/3 else \"right\" if cx > 2*w/3 else \"center\"\n","\n","        ver = \"top\"  if cy < h/3 else \"bottom\" if cy > 2*h/3 else \"center\"\n","\n","        if hor==\"center\" and ver==\"center\":\n","            rel = \"center\"\n","        elif hor==\"center\":\n","            rel = ver + \"-center\"\n","        elif ver==\"center\":\n","            rel = \"center-\" + hor\n","        else:\n","            rel = ver + \"-\" + hor\n","        result.append((lab, [round(x,1) for x in box], rel))\n","    return result"],"metadata":{"id":"twvXoF9axiRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TextStreamer\n","\n","def evaluate_with_fastvision(img_path: str):\n","\n","    dets = get_detections_with_pos(img_path)\n","\n","\n","    if not dets:\n","        instruction = \"No necrotizing soft tissue infection lesions were detected in this image.\"\n","    else:\n","        mapping = {\n","            \"air\": \"ectopic gas\",\n","            \"thick\": \"fascia edematous changes\",\n","            \"water\": \"fluid accumulation\",\n","            \"low attenuation\": \"soft tissue non-enhancement\"\n","        }\n","        lines = [\"Below are the detections found by the model:\"]\n","        for label, coords, rel in dets:\n","            desc = mapping.get(label, label)\n","            lines.append(f\"- {label} ({desc}), coordinates: {coords}, relative position: {rel}\")\n","\n","        lines.append(\n","            \"\\nNow **for each** of the above detections, \"\n","            \"write **one complete English sentence** describing the finding, its bounding box, \"\n","            \"and its relative location. \"\n","            \"Make sure you cover **all** detections, **one sentence per detection**, each on its own line.\"\n","        )\n","        instruction = \"\\n\".join(lines)\n","\n","\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"image\", \"image\": Image.open(img_path).convert(\"RGB\")},\n","            {\"type\": \"text\",  \"text\": instruction}\n","        ]\n","    }]\n","\n","\n","    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n","    inputs = tokenizer(\n","        Image.open(img_path).convert(\"RGB\"),\n","        input_text,\n","        add_special_tokens=False,\n","        return_tensors=\"pt\",\n","    ).to(model.device)\n","\n","\n","    streamer = TextStreamer(tokenizer, skip_prompt=True)\n","    with torch.inference_mode():\n","        _ = model.generate(\n","            **inputs,\n","            streamer=streamer,\n","            max_new_tokens=256,\n","            use_cache=True,\n","            do_sample=False,\n","        )\n"],"metadata":{"id":"qoG9c7aVxovO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ct_folder = Path(\"LLM_2\")\n","for img_path in sorted(ct_folder.glob(\"*.*\")):\n","    if img_path.suffix.lower() not in {\".png\", \".jpg\", \".jpeg\"}:\n","        continue\n","\n","\n","    res = det_model(str(img_path))[0]\n","    annotated = res.plot()\n","    annotated_img = Image.fromarray(annotated)\n","\n","\n","    display(annotated_img)\n","\n","\n","    print(f\"\\n=== {img_path.name} Analysis ===\")\n","    evaluate_with_fastvision(str(img_path))\n","\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n"],"metadata":{"id":"hqT-TgVB4flD"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}